---
title: "statlearning_hw1"
author: '110071010'
date: "2024-09-30"
output:
  word_document: default
---

# Problem Set 8 (p54)

## (a)

Use the read.csv() function to read the data into R. Call the loaded data college. Make sure that you have the directory set to the correct location for the data

```{r}
# Read in data
college <- read.csv("https://www.statlearning.com/s/College.csv")
head(college)
```

## (b)

Look at the data using the View() function. This loads a matrix or data.frame object into the spreadhseet-like viewer in RStudio, just clicking the name of the object will do in the Environment panel.

```{r}
View(college)
```

You should notice that the first column is just the name of each university. We don’t really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands:

```{r}
rownames(college) <- college[,1]
fix(college)
```

You should see that there is now a row.names column with the name of each university recorded. This means that R has given each row a name corresponding to the appropriate university. R will not try to perform calculations on the row names. However, we still need to eliminate the first column in the data where the names are stored. Try

```{r}
college <- college[,-1]
fix(college)
```

Now you should see that the first data column is Private. Note that another column labeled row.names now appears before the Private column. However, this is not a data column but rather the name that R is giving to each row

## (c)

### i

Use the summary() function to produce a numerical summary of the variables in the data set.

```{r}
summary(college)
```

### ii

Use the `pairs()` function to produce a scatterplot matrix of the first ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix `A` using `A[,1:10]`

```{r fig.width=12, fig.height=9}
# First change Private to a factor variable, and change levels from "Yes/No" to "Private/Public"
college$Private <- as.factor(college$Private)
levels(college$Private) <- c("Public","Private")

# Using pairs()
pairs(college[,1:10])
```

### iii

Use the `plot()` function to produce side-by-side boxplots of `Outstate` versus `Private`.

```{r fig.width=8, fig.height=6}
plot(college$Private, college$Outstate,
     xlab = "University", ylab = "Tuition in USD")
```

### iv

Create a new qualitative variable, called `Elite`, by binning the `Top10perc` variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%.

```{r}
Elite <- rep("No", nrow(college))
Elite[college$Top10perc > 50] <- "Yes"
Elite <- as.factor(Elite)
college <- data.frame(college, Elite)
```

Use the `summary()` function to see how many elite universities there are. Now use the `plot()` function to produce side-by-side boxplots of `Outstate` versus `Elite`.

```{r}
summary(Elite)
```

```{r fig.width=8, fig.height=6}
plot(college$Elite, college$Outstate,
     xlab = "Elite Univeristy", ylab = "Tuition in USD")
```

### v

Use the `hist()` function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command `par(mfrow=c(2,2))` useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.

```{r fig.width=8, fig.height=6}
par(mfrow=c(2,2))

# different bin num on Applications Received
hist(college$Apps,breaks=20, xlab = "Applications Received", main = "")
hist(college$Apps,breaks=100, xlab = "Applications Received", main = "")

# different bin num on Student/faculty ratio
hist(college$S.F.Ratio, breaks=20, xlab = "Student/faculty ratio", main = "")
hist(college$S.F.Ratio, breaks=100, xlab = "Student/faculty ratio", main = "")
```

### vi

Continue exploring the data, and provide a brief summary of what you discover.

```{r}
model <- lm(Grad.Rate ~ Outstate + Top10perc + Room.Board, data = college)
summary(model)
```

[**Summary**]{.underline}

Suppose the model to be linear. Then Outstate, Top10perc, and Room.Board are found to have statistically significant relationships with Grad.Rate, with all p-values smaller than 0.05.

# Problem Set 10 (p56)

## (a)

```{r}
# Load in the Boston data set (part of the MASS library in R)
library(MASS)
head(Boston)
```

```{r}
# Read about the data set
?Boston
```

[**Question**]{.underline}

How many rows are in this data set? How many columns? What do the rows and columns represent?

[**Answer**]{.underline}

```{r}
dim(Boston)
```

The Boston data frame has 506 median values of owner-occupied homes in \$1000s (rows) with 14 features each (columns).

## (b)

Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your finding.

```{r}
# Inspect the structure of the Boston dataset
str(Boston)
```

```{r}
# Convert ints into num
Boston$chas <- as.numeric(Boston$chas)
Boston$rad <- as.numeric(Boston$rad)
```

```{r fig.width=12, fig.height=9}
# Create pairwise scatterplot
pairs(Boston)
```

[**Comment**]{.underline}

lstat (percentage of lower status population) seems to correlate negatively with medv, suggesting that areas with a higher proportion of lower-status residents tend to have lower home values.

## (c)

Are any of the predictors associated with per capita crime rate? If so, explain the relationship.

```{r}
cor(Boston)
```

```{r fig.width=8, fig.height=6}

par(mfrow=c(2,2))
plot(crim ~ age, data = Boston, log = "xy")
plot(crim ~ dis, data = Boston, log = "xy")
plot(crim ~ rad, data = Boston, log = "xy")
```

[**Comment**]{.underline}

I found three relationships that seem attributable to increased crime rates:

-   older homes (age ↑)

-   closer to work-area (distance ↓)

-   closer to radial highway (rad ↑)

## (d)

Do any of the suburbs of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.

```{r fig.width=8, fig.height=6}
par(mfrow=c(2,2))

# crime rates
hist(Boston$crim[Boston$crim > 1], breaks=25)

# tax rates
hist(Boston$tax, breaks=25)

# pupil-teacher ratios
hist(Boston$ptratio, breaks=25)
```

[**Comment**]{.underline}

-   **Crime rates:** 18 suburbs have a crime rate greater than 20, with the highest values reaching above 80.

-   **Tax rates:** a distinct peak between **660** and **680.**

-   **Pupil-teacher ratios**: has notably the highest freq at 20.25

# Problem Set 8 (p121)

This question involves the use of simple linear regression on the Auto data set.

## (a)

Use the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summary() function to print the results. Comment on the output.

```{r}
# Load in necessary data set
library(ISLR)
data("Auto")
head(Auto)
```

```{r}
# Fit the model
model <- lm(mpg ~ horsepower, data = Auto)
summary(model)
```

### i

[**Question**]{.underline}

Is there a relationship between the predictor and the response?

[**Answer**]{.underline}

Yes. The p-value (2e-16) is small enough for one to claim a significant relationship between them.

### ii

[**Question**]{.underline}

How strong is the relationship between the predictor and the response?

[**Answer**]{.underline}

The R-squared value suggests around 61% of the variation in mpg (response) can be explained by horsepower (predictor).

### iii

[**Question**]{.underline}

Is the relationship between the predictor and the response positive or negative?

[**Answer**]{.underline}

Negative (see sign of the horsepower coefficient)

### iv

What is the predicted mpg associated with a horsepower of 98? What are the associated 95% confidence and prediction intervals?

```{r}
# Create a new data frame for prediction
new_data <- data.frame(horsepower = 98)

# Get predictions along with intervals
predictions <- predict(model, newdata = new_data, interval = "confidence", level = 0.95)
predictions_with_prediction <- predict(model, newdata = new_data, interval = "prediction", level = 0.95)

cat("Confidence Interval:\n")
print(predictions)
cat("\nPrediction Interval:\n")
print(predictions_with_prediction)
```

## (b)

Plot the response and the predictor. Use the abline() function to display the least squares regression line.

```{r fig.width=8, fig.height=6}
plot(Auto$horsepower, Auto$mpg, main = "Scatterplot of mpg vs horsepower",
     xlab = "horsepower", ylab = "mpg", 
     col = "cornflowerblue", pch =19 )
# Add the regression line
abline(model, col = "darkblue", lwd = 2)
```

## (c)

Use the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.

```{r fig.width=8, fig.height=6}
par(mfrow = c(2, 2))
plot(model)
```

[**Comment**]{.underline}

The plot of residuals versus fitted values (upper-left) shows signs of non linearity within the data.

# Problem Set 9 (p122)

This question involves the use of multiple linear regression on the Auto data set.

## (a)

Produce a scatterplot matrix which includes all of the variables in the data set.

```{r fig.width=12, fig.height=9}
library(ISLR)

# Create a scatterplot matrix
pairs(Auto)
```

## (b)

Compute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, which is qualitative.

```{r}
# View(Auto)
cor(Auto[, -which(names(Auto) == "name")])
```

## (c)

Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output. For instance:

```{r}
# Fit the model
model <- lm(mpg~.-name, data = Auto)
summary(model)
```

### i

[**Question**]{.underline}

Is there a relationship between the predictors and the response?

[**Answer**]{.underline}

Yes. The relationship can be evidenced by testing the null hypothesis of whether all the regression coefficients are zero. And it turns out that the F-statistic is far from 1 with small p-value (2.2-16).

### ii

[**Question**]{.underline}

Which predictors appear to have a statistically significant relationship to the response?

[**Answer**]{.underline}

P-values corresponding to each predictor's t-statistic reveals that displacement, weight, year, and origin have statistically significant relationships with the response, while cylinders, horsepower, and acceleration do not.

### iii

[**Question**]{.underline}

What does the coefficient for the "year" variable suggest?

[**Answer**]{.underline}

The regression coefficient for year (0.7507727) indicates for every one year, mpg increases by the coefficient, namely that cars become more fuel-efficient every year by roughly 1 mpg per year.

## (d)

Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?

```{r fig.width=8, fig.height=6}
par(mfrow = c(2, 2))
plot(model)
```

[**Comment**]{.underline}

**QQ Residual Plot:** Some points fall above the line in the tails, suggesting that there are more extreme values (outliers) than expected under a normal distribution.

**Leverage Plot:** Points with high leverage and large residuals can be problematic because they suggest that the model might not fit well for those observations. Point 14 stands out in this sense.

## (e)

Use the \* and : symbols to fit linear regression models with interaction effects.ffects.

```{r}
# Consider interaction effects
interaction_model <- lm(mpg ~ cylinders * displacement + displacement * weight, data = Auto)
summary(interaction_model)
```

[**Question**]{.underline}

Do any interactions appear to be statistically significant?

[**Answer**]{.underline}

According to the chart presented above, displacement: weight seems statistically significant with small-enough p-value, while cylinders:displacement does not.

# Problem Set 10 (p123)

This question should be answered using the Carseats data set.

## (a)

Fit a multiple regression model to predict Sales using Price, Urban and US.

```{r}
# Load the data set
data(Carseats)

# Fit the model
model1 <- lm(Sales ~ Price + Urban + US, data = Carseats)
summary(model1)
```

## (b)

[**Question**]{.underline}

Provide an interpretation of each coefficient in the model. Be careful, some of the variables in the model are qualitative!

[**Comment**]{.underline}

-   **Price**: For each one-unit increase in `Price`, the `Sales` are expected to decrease by approximately 0.0545 units, holding `Urban` and `US` constant. This coefficient is statistically significant (p \< 2e-16), indicating a strong relationship.

-   **UrbanYes**: suggests that being in an urban area is associated with a decrease in sales of around 0.0219 units, holding other variables constant. However, this predictor is [not statistically significant]{.underline} (p = 0.936), indicating that it does not provide meaningful information about Sales.

-   **nYes:** Stores in the US have average sales that are 1.200573 units higher than those not in the US, holding `Price` and `Urban` constant. This predictor is statistically significant (p = 4.86e-06), indicating a strong positive relationship.

## (c)

[**Question**]{.underline}

Write out the model in equation form, being careful to handle the qualitative variables properly.

[**Answer**]{.underline}

Sales = 13.04 + -0.05 Price + -0.02 UrbanYes + 1.20 USYes

## (d)

[**Question**]{.underline}

For which of the predictors can you reject the null hypothesis H0: βj = 0 ?

[**Answer**]{.underline}

The F-statistic is high (41.52) with a corresponding p-value much less than 0.05, allowing us to reject the null hypothesis that all coefficients are zero. This indicates that at least one predictor (Price, UrbanYes, or USYes) significantly affects Sales.

-   **Price:** p \< 2e-16 (reject H0)

-   **UrbanYes:** p = 0.936 (fail to reject H0)

-   **USYes:** p = 4.86e-06 (reject H0)

It follows that one can reject the null hypothesis for Price and USYes, indicating these predictors have a statistically significant effect on Sales.

## (e)

On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.

```{r}
# Fit a smaller model using only significant predictors
model2 <-  lm(Sales ~ Price + US, data = Carseats)
summary(model2)
```

## (f)

[**Question**]{.underline}

How well do the models in (a) and (e) fit the data?

[**Answer**]{.underline}

Both models have the same R² value of **0.2393**, indicating they explain approximately **23.93%** of the variance in Sales. However, the reduced model (e) has a slightly better fit with a lower RSE of **2.469** compared to **2.472** for the full model (a).

## (g)

Using the model from (e), obtain 95% confidence intervals for the coefficient(s).

```{r}
# 95% confidence intervals for the coefficients in model2
confint(model2, level = 0.95)
```
